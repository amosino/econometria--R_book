# El modelo de regresión simple

Nota que la fórmula para $y_i$ que hemos aprendido (ecuación \@ref(eq:mrs) de la sección anterior) tiene dos componentes: 1) parte no aleatoria y 2) parte aleatoria.

La parte no aleatoria puede definirse como:

\begin{equation}
\mathbb{E}(y_i | x_i) = \beta_1 + \beta_2 x_i.
\end{equation}

La fórmula anterior implica que, si conociéramos los valores de $\beta_1$ y $\beta_2$, la línea que estamos buscando partiría de $\beta_1$ y tendría pendiente $\beta_2$.

Por otra parte, el componente aleatorio es $u_i$. En otras palabras, $u_i$ es un componente desconocido que no nos permite calcular con certeza los valores de $y_i$ a partir de la ecuación \@ref(eq:mrs). 

Pero, ¿porqué existe este $u_i$?

La inclusión de $u_i$ en el modelo se justifica por las siguientes razones:

* La posible omisión de variables explicativas.
* Una posible agregación incorrecta de variables.
* Una posible especificación incorrecta del modelo.
* La posible elección de una forma funcional incorrecta.
* Por posibles errores de medición.

Lo anterior implica que, dado que no es posible encontrar la línea _verdadera_ delimitada por $\mathbb{E}(y_i)$, esta tiene que ser aproximada. Esta aproximación resulta en la siguiente ecuación _estimada_:


\begin{equation}
\widehat{y}_i= b_1 + b_2 x_i.
\end{equation}

Aunque esta línea podría bien trazarse a _ojo de buen cubero_, existen algunas técnicas formales, todas ellas involurando al _residual de estimación_. Este está definido como:

\begin{align}
e_i &= y_i - \widehat{y}_i \\
    &= y_i - b_1 - b_2 x_i (\#eq:mrs-res)
\end{align}

Naturalmente, el objetivo es encontrar  $b_1$ y $b_2$ tal que los valores de $e_i$ sean los más bajos posibles.

## Minimización de $\sum_{i=1}^{N}e_i$

Como primera opción para estimar los coeficientes de la ecuación \@ref(eq:mrs-res), podríamos intentar resolver el siguiente problema:

\begin{align}
\min_{b_1,b_2}\sum_{i=1}^{N}e_i &= \min_{b_1,b_2}\sum_{i=1}^{N}(y_i - \widehat{y}_i)\\
&= \min_{b_1,b_2}\sum_{i=1}^{N}(y_i - y_i - b_1 - b_2 x_i).
\end{align}

Sin embargo, notemos que si fijamos:

\begin{align}
b_1 &= \overline{y}=\frac{1}{N} \sum_{i=1}^{N} y_i\\
b_2 &= 0,
\end{align}
obtenemos que:

\begin{equation}
\sum_{i=1}^{N}e_i = 0,
\end{equation}
es decir, la suma de los residuales es cero (su valor mínimo) y:
\begin{equation}
\widehat{y}_i = \overline{y}.
\end{equation}
El resultado anterior implica que siempre podríamos obtener una línea horizontal sobre $\overline{y}$. Esto, naturalmente, no parece ser una buena aproximación, sean cuales sean los datos analizados. Para nuestro ejemplo de la esperanza de vida, esta aproximación resultaría en la siguiente línea de estimación y coeficientes:

```{r}
data_full<-read.csv(file.path("/Users","alejandro.mosino","Documents","github","econometria--R_book","_data_econometria","lif_exp.csv"), header=TRUE, sep=",")
b1 <- mean(data_full$life_exp)
b2 <- 0
with(data_full,plot(rgdp,life_exp,xlab="Ingreso per cápita", ylab="Esperanza de vida"))
abline(a=b1, b=b2)
print(paste0("El valor estimado b1 es: ", b1))
print(paste0("El valor estimado b2 es: ", b2))
```

Compara este resultado con el presentado en la sección anterior. ¿Cuál te parece mejor?

## El método de los mínimos cuadrados ordinarios: introducción

El resultado de la sección anterior puede mejorarse si, en lugar de minimizar la suma de los residuales, minimizamos la _suma del cuadrado de los residuales_ (RSS por sus siglas en inglés). Este problema se conoce como el __método de los mínimos cuadrados ordinarios__ (MCO u OLS por sus siglas en inglés), y requiere encontrar los estimadores $b_1$ y $b_2$ tales que: 

\begin{align}
b_1 , b_2 &= \underset{b_1,b_2}{\arg\min}\left\{RSS\right\} \\
RSS&= \sum_{i=1}^N e_i^2.
\end{align}

Para ver el funcionamiento de este método, consideremos el siguiente ejemplo:

```{r mrsex1-tab, tidy=FALSE}
x<-c(1,2)
y<-c(3,5)
yhat<-c("$b_1+b_2$", "$b_1+2b_2$")
e<-c("3-$b_1-b_2$", "$5- b_1-2b_2$")
lab<-c("$x_i$", "$y_i$", "$yhat_i$", "$e_i$")
data_full<-data.frame(x,y,yhat,e)
names(data_full)<-lab
knitr::kable(
data_full[,1:2], caption = 'Ejemplo: Mínimos cuadrados ordinarios. Dos observaciones.',
booktabs = TRUE
)
```

Para encontrar los estimadores del método de los MCO, seguimos los siguientes pasos:

* Primero, calculamos $e_i$. Recordemos que este está definido como $e_i=y_i-\widehat{y}_i$. Los cálculos se muestran en la siguiente tabla:

```{r mrsex1e-tab, tidy=FALSE}

knitr::kable(
data_full, caption = 'Ejemplo: Mínimos cuadrados ordinarios. Cálculo del residual.',
booktabs = TRUE
)
```

* Ahora podemos calcular el RSS. Este es:

\begin{equation}
RSS= (3-b_1-b_2)^2+(5-b_1-2b_2)^2
\end{equation}

* Minimizamos el $RSS$ con respecto a $b_1$ y $b_2$. Las condiciones de primer orden resultan en el siguiente sistema de dos ecuaciones con dos incógnitas:

\begin{align}
2b_1+3b_2-8&=0\\
3b_1+5b_2-13&=0.
\end{align}

* Ahora Resolvemos el sistema. Resulta muy fácil encontrar que:

\begin{align}
b_1 &= 1\\
b_2 &= 2.
\end{align}

* Finalmente, estamos listos para trazar la línea estimada. Esta se ve como en la siguiente figura:

```{r mrsex1-fig, fig.cap='Solución por MCO. Datos ficticios', out.width='80%', fig.asp=.75, fig.align='center'}
with(data_full,plot(x,y, xlab="x", ylab="y"))
with(data_full,abline(lm(y ~ x)))
```

**Ejercicio**

¿Cómo cambiaría el resultado anterior si agregamos un tercer punto con coordenadas $(x_3,y_3)=(3,6)$?

## El método de los mínimos cuadrados ordinarios

El método de la sección anterior puede generalizarse para cualquier número de observaciones, $N$. En general, el problema consiste en:

\begin{align}
\min_{b_1,b_2} RSS &= \min_{b_1,b_2} \sum_{i=1}^N e_i^2 \\
&=\min_{b_1,b_2} \sum_{i=1}^N (y_i - \widehat{y})^2 \\
&=\min_{b_1,b_2} \sum_{i=1}^N (y_i - b_1 - b_2x_i)^2.
\end{align}

Las condiciones de primer orden de este problema se conocen como _ecuaciones normales de los coeficientes de regresión_:

\begin{align}
N b_1 - \sum_{i=1}^N y_i + b_2 \sum_{i=1}^N x_i &= 0 \\
b_2 \sum_{i=1}^N x_i^2 - \sum_{i=1}^N x_i y_i + b_1 \sum_{i=1}^N x_i &=0. 
\end{align}

La solución a este sistema resulta en los _estimadores de MCO_ para $\beta_1$ y $\beta_2$. Estos son:

\begin{align}
b_1 &= \overline{y}-b_2 \overline{x}\\
b_2 &= \frac{\sum_{i=1}^N(x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^N(x_i - \overline{x})^2}. (\#eq:est-mco)
\end{align}

**Ejercicio**

Deriva las ecuaciones en el sistema \@ref(eq:est-mco).

**Ejercicio**

Considera los siguientes datos que miden la relación entre el promedio escolar y el ingreso familiar:

```{r}
data_ex<-read.csv(file.path("/Users","alejandro.mosino","Documents",
"github","econometria--R_book","_data_econometria","prom_ing.csv"), header=TRUE, sep=",")
print(data_ex)
```

Utiliza las ecuaciones del sistema \@ref(eq:est-mco) para encontrar los estimadores de MCO, $b_1$ y $b_2$. Comenta.

**Ejercicio**

Muestra que $b_2$ en el sistema \@ref(eq:est-mco) también puede escribirse como:

* $b_2 = \frac{\text{cov}(x,y)}{\text{var}(x)}$
* $b_2 = \frac{\sum_{i=1}^N x_i y_i - N \overline{x}\overline{y}}{\sum_{i=1}^N x_i^2 - N \overline{x}^2}$
* $b_2 = \frac{\sum_{i=1}^N (x_i -\overline{x}) y_i}{\sum_{i=1}^N (x_i - \overline{x}^2)}$

**Ejercicio**

Muestra que:

* $\overline{e} = 0$
* $\overline{\widehat{y}_i} = \overline{y}$
* $\text{cov}(\widehat{y}_i,e_i)=0$
